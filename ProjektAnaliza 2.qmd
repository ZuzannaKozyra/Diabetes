---
title: "Projekt z zakresu analizy danych"
author: "Zuzanna Kozyra, Michał Łysakowski"
format: 
  html:
    warning: false
    message: false
    echo: false
    self-contained: true
    toc: true
    toc-location: left
    toc-title: Spis treści
editor: visual
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
---

# Cel Projektu

Badanie wpływu różnych czynników na zachorowania na cukrzyce. Zbiór danych pochodzi z instytutu ['National Institute of Diabetes and Digestive and Kidney Diseases'](https://www.niddk.nih.gov/ "strona instytutu"). W próbie wszystkie pacjentki to kobiety pochodzące z Indii, w wieku 21 lat lub starsze.

```{r}
library(knitr)
library(tidyverse)
library(Hotelling)
library(rstatix)
library(dplyr)
library(DT)
library(ggpubr)
library(ggcorrplot)
library(vip)
library(gridExtra)
library(readr)
library(forcats)
library(rstatix)
library(tidymodels)
library(MASS)
library(Metrics)
library(plotly)
library(rpart.plot)
```

# Opis zbioru danych

## Wczytanie danych

```{r}
#| tbl-cap: "Zbiór danych Diabetes"
#| label: tbl-data

dane <- read.csv('diabetes.csv')
dane <- dane %>% 
  add_column (Id = 1: nrow(dane), .before = 1)
datatable(dane[,-1])
```

## Opis zmiennych

Zmienne podane są w zbiorze `diabetes` w języku angielskim, poniżej znajduje się tabela opisująca każdą zmienną.

```{r}
#| tbl-cap: "Opis zmiennych"
#| label: tbl-opis


library(knitr)

opis <-  data.frame(matrix(c('Id', 'Numer wiersza/obserwacji',
                                'Pregnacies','Ilość przebytych ciąż',

                                'Glucose', 'Plazmowe stężenie glukozy po upływie 2 godzin w teście tolerancji glukozy doustnej',

                                'BloodPressure', 'Rozkurczowe ciśnienie krwi',

                                'SkinThickness', 'Grubość fałdu skórnego tricepsa (mm)',

                                'Insulin', 'Poziom insuliny',

                                'BMI', 'Body Mass Index, wskaźnik masy ciała (waga w kg/(wzrost w m)^2)',

                                'DiabetesPedigreeFunction', 'Funkcja obliczająca prawdopodobieństwo wystąpienia cukrzycy biorąca pod uwagę wiek oraz historie chorób w rodzinie pacjenta',
                             'Age', 'Wiek (w latach)',
                             'Outcome', 'Wynik, 1 gdy cukrzyca wystąpiła, 0 gdy cukrzyca nie wystąpiła'),ncol = 2, byrow = T))

colnames(opis) <- c("Oryginalna nazwa zmiennej", "Wyjaśnienie")

kable(opis)
```

# Sprawdzanie poprawności zbioru danych

## Występowanie braków danych

```{r}
is.na(dane) %>%

  sum()
```

Brak braków danych.

## Występowanie duplikatów

```{r include=FALSE}
duplikaty <- unique(dane)
length(dane$Pregnancies) == length(duplikaty$Pregnancies)
```

Brak duplikatów danych (ilość unikalnych wartości jest taka sama jak w pierwotnej tabeli).

## Poprawność zmiennych

```{r}
#| tbl-cap: "Podstawowe statystyki"
#| label: tbl-stat1

kable(summary(dane[,-c(1,10)]))
```

Patrząc na statystyki możemy zauważyć już pewne nieścisłości, które z medycznego punktu widzenia nie mogą zachodzić u człowieka.

1.  `Pregnancies` = 17

    ```{r}
    kable(dane[dane$Pregnancies == 17,])
    ```

    Według tej obserwacji (numer 160) kobieta w wieku 47 lat przebyła 17 ciąż, więc zostaje ona usunięta ze zbioru.

    ```{r}
    dane <- dane[-160,]
    ```

2.  `Glucose` = 0

    ```{r}
    id_g <- dane %>%
      filter(Glucose == 0)

    dane <- dane %>%
      filter(!Id %in% id_g$Id)
    ```

    Na podstawie: @jeznach-steinhagen_zywienie_2020

3.  `BloodPressure` = 0

    Na podstawie: @jakubaszko_kirschnik_pielegniarstwo_1997

    ```{r}
    id_bp <- dane %>%
      filter(BloodPressure == 0)

    dane <- dane %>%
      filter(!Id %in% id_bp$Id)
    ```

4.  `SkinThickness` = 0

    Na podstawie: @malinowski_bozilow_podstawy_1997

    ```{r}
    id_st <- dane %>%
      filter(SkinThickness == 0)

    dane <- dane %>%
      filter(!Id %in% id_st$Id)
    ```

5.  `Insulin` = 846 oraz `Insulin` = 0

    Na podstawie: @gajewski_interna_2022

    ```{r}
    id_i846 <- dane %>%
      filter(Insulin == 846)

    dane <- dane %>%
      filter(!Id %in% id_i846$Id)

    id_i0 <- dane %>%
      filter(Insulin == 0)

    dane <- dane %>%
      filter(!Id %in% id_i0$Id)
    ```

6.  `BMI` = 0

    Na podstawie: @stupnicki_wskaznik_2016

    ```{r}
    id_bmi <- dane %>%
      filter(BMI == 0)

    dane <- dane %>%
      filter(!Id %in% id_bmi$Id)
    ```

Wyżej wymienione przypadki zostają usunięte ze zbioru danych.

## Występowanie obserwacji odstających

```{r}
#| fig-cap: "Wykresy pudełkowe danych"
#| label: fig-box

dane1 <- scale(dane[,-c(1,10)], center = F)

boxplot(dane1[,c(-1,-10)], las = 2)

```

Zmienne zostały poddane standaryzacji, dzięki czemu mogły one zostać przedstawione na jednym wykresie. Z @fig-box można wynioskować, że należy zbadać występowanie obserwacji odstających.

### Wielowymiarowe elementy odstające

```{r}
#| tbl-cap: "Elementy odstające" 
#| label: tbl-outliers

tbl <- dane %>% 
  group_by(Outcome) %>% 
  mahalanobis_distance(-Id) %>%
  as.data.frame() %>% 
  filter(is.outlier == TRUE)

kable(tbl)
```

Korzystając z odległości Mahalanobisa zidentyfikowane zostały nietypowe obserwacje. W tabeli @tbl-outliers przedstawione są wielowymiarowe elementy odstające, które zostają usunięte ze zbioru danych, by nie zaburzać statystyk oraz parametrów.

```{r}
dane <- dane %>% 
  filter(!Id %in% tbl$Id)
```

# Analiza zbioru danych

## Podstawowe statystyki opisowe

```{r}
#| tbl-cap: "Podstawowe statystyki"
#| label: tbl-stat2

kable(summary(dane[, 2:9]))
```

W @tbl-stat2 przedstawione zostały podstawowe statystyki już po usunięciu obserwacji nieprawidłowych oraz odstających. Możemy z niej odczytać między innymi średnie wartości zmiennych ze zbioru. Widać, że zmienne po wstępnym czyszczeniu zbioru średnio przyjmują wartości zgodne z wiedzą medyczną.

Z tabeli można również odczytać, że:

-   Większość pacjentek miała 0, 1 lub 2 ciąże, mediana wynosi 2, a najwięcej przebytych ciąż dla jednej pacjentki to aż 15

-   Mediana zmiennej `Glucose` jest zbliżona do średniej, więc można przypuszczać, że ma rozkład zbliżony do symetrycznego

-   To samo można powiedzieć o wszystkich pozostałych zmiennych

-   Większość pacjentek jest w wieku do 30 lat

## Korelacje

```{r}
#| fig-cap: "Wizualizacja macierzy korelacji"
#| label: fig-cormat

library(ggcorrplot)
library(rstatix)

dane$Pregnancies <- as.numeric(dane$Pregnancies)
dane$Glucose <- as.numeric(dane$Glucose)
dane$BloodPressure <- as.numeric(dane$BloodPressure)
dane$SkinThickness <- as.numeric(dane$SkinThickness)
dane$Insulin <- as.numeric(dane$Insulin)
dane$Age <- as.numeric(dane$Age)
dane$Outcome <- as.numeric(dane$Outcome)
ggcorrplot(cor_mat(subset(dane, select = -c(Id, Outcome))), p.mat=corr_pmat(subset(dane, select = -c(Id, Outcome))), lab=F, colors = c("lightblue", "skyblue", "darkblue"))
```

Patrząc na wykres macierzy korelacji widać, że silnie skorelowane pary zmiennych to: `Pregnancies` i `Age`, `Insulin` i `Glucose`, `SkinThickness` i `BMI`, a zmienna `DiabetesPedigreeFunction` nie koreluje z pozostałymi.

```{r}
#| tbl-cap: "Macierz korelacji"
#| label: tbl-cormat

kable(cor_mat(subset(dane, select = -c(Id, Outcome))))
```

@tbl-cormat potwierdza powyższe związki. Wraz ze wzrostem zmiennej `Age` (wiek) rośnie również `Pregnancies` (ilość przebytych ciąż), tak samo w przypadku, gdzie wartość korelacji jest bliska jeden oraz dodatnia. Pozostałe zmienne nie są silnie skorelowane - nie ma między nimi zależności liniowej.

## Analiza związków między zmiennymi

### Histogramy zmiennych

```{r}
#| fig-cap: "Histogramy zmiennych"
#| label: fig-hist_all

par(mfrow=c(3, 3), mar=c(4, 4, 2, 1), oma=c(0, 0, 2, 0))

for (i in 1:length(subset(dane, select = -c(Id, Outcome)))) {
  hist(subset(dane, select = -c(Id, Outcome))[, i], main=names(subset(dane, select = -c(Id, Outcome)))[i], col="skyblue", xlab="", ylab="", border="black")
}
```

Na podstawie histogramów można podejrzewać, że większa część obserwacji ma małą liczbę ciąż, niski poziom insuliny oraz DiabetesPedigreeFunction i jest młodsza. Grubość skóry i BMI wydają się rozkładać w miarę równomiernie wokół średniej.

### Zmienna `Outcome`

```{r}
#| fig-cap: "Wykres kołowy zmiennej Outcome"
#| label: fig-piechart

out <- plyr::count(dane, 'Outcome')

ggplot(out, aes(x = "", y = freq, fill = factor(Outcome))) +
  geom_bar(width = 1, stat = "identity") +
  geom_text(aes(label = paste0(round(freq/sum(freq) * 100, 1), "%")),
            position = position_stack(vjust = 0.5),
            color = "white") +
  coord_polar(theta = "y") +
  theme_void() +
  scale_fill_manual(values = c("0" = "skyblue", "1" = "darkblue")) +
  labs(fill = "Outcome")
```

```{r}
#| tbl-cap: "Ilość obserwacji w podziale na Outcome"
#| label: tbl-freq

freq <- data.frame(plyr::count(dane, 'Outcome'))
colnames(freq) <- c("Outcome", "Ilość wystąpień")
kable(freq)
```

Dla zmiennej `Outcome` wyniki zostały przedstawione w postaci wykresu kołowego @fig-piechart. W taki sposób widać, że ponad dwa razy więcej w tym zbiorze jest kobiet, które nie mają cukrzycy - jest ich aż 258.

### Wiek w podziale na `Outcome`

```{r}
cukrzycy <- dane[dane$Outcome == 1,]

zdrowi <- dane[dane$Outcome == 0,]
```

```{r}
#| fig-cap: "Rozkład wieku w podziale na zmienną Outcome"
#| label: fig-box_age_outcome

dane %>%  
  ggboxplot(x = "Outcome", y = "Age", combine = 1, color = "Outcome", add = "jitter")+
  scale_color_manual(values = c("darkblue", "skyblue"), labels = c("0", "1"))+
  scale_x_discrete(labels = c("0", "1")) +
  theme_minimal()
```

Z wykresu pudełkowego można odczytać, że cukrzyca występuje u osób zazwyczaj po 30 roku życia. Zaś u osób młodszych (między 20 a 30 rokiem życia) rzadziej diagnozowano cukrzycę.

```{r}
#| tbl-cap: "Średni wiek w podziale na Outcome"
#| label: tbl-age_outcome

age_outcome <- data.frame(mean(cukrzycy$Age), mean(zdrowi$Age))
colnames(age_outcome) <- c("Średni wiek cukrzyka", "Średni wiek osoby zdrowej")
kable(age_outcome)
```

Obliczenia potwierdzają poprzednie przypuszczenia.

### Ciąże w podziale na `Outcome`

```{r}
#| fig-cap: "Wykres pudełkowy zmiennej Pregnancies z podziałem na Outcome"
#| label: fig-preg_outcome

dane %>%  
  ggboxplot(x = "Outcome", y = "Pregnancies", combine = 1, color = "Outcome", add = "jitter")+
  scale_color_manual(values = c("darkblue", "skyblue"), labels = c("0", "1"))+
  scale_x_discrete(labels = c("0", "1")) +
  theme_minimal()
```

Różnica w ilości ciąż w podziale na grupy jest nie aż tak duża jak przy `Age`.

```{r}
#| tbl-cap: "Średnia ilość ciąż w podziale na Outcome"
#| label: tbl-preg_outcome

preg_outcome <- data.frame(mean(cukrzycy$Pregnancies), mean(zdrowi$Pregnancies))
colnames(preg_outcome) <- c("Średnia ilość ciąż cukrzyka", "Średnia ilość ciąż osoby zdrowej")
kable(preg_outcome)
```

Ponownie, to, co można było podejrzewać na podstawie wykresu, obliczenia potwierdziły.

### `Age` i `Pregnancies`

```{r}
#| fig-cap: "Wykres punktowy zależności wieku od ilości ciąż"
#| label: fig-preg_age

dane %>% 
  ggplot(aes(x = Pregnancies, y = Age))+
  geom_point(color = "darkblue")+
  theme_minimal()
```

Korelacja między `Age` i `Pregnancies` wynosi 0.72, co jest największą wartością korelacji między zmiennymi w tym zbiorze danych. Wykres punktowy nie sugeruje wprost zależności liniowej pomiędzy tymi zmiennymi.

Jako rozszerzenie badania korelacji między dwiema zmiennymi stosuje się również model regresji liniowej.

```{r}
age_preg <- lm(Age ~ Pregnancies, dane)
summary(age_preg)
```

Zmienna `Pregnancies` jest istotna statystycznie przy wyjaśnianiu zmienności zmiennej `Age`. Jednak $R^2$ wynosi jedynie 51%, czyli za pomocą tego modelu można wyjaśnić jedynie 51% zmienności zmiennej `Age`.

Warto przetestować hipotezę: $H_0$: zależność między `Age` a `Pregnancies` jest liniowa.

```{r}
library(lmtest)
raintest(age_preg)
resettest(age_preg)
harvtest(age_preg)
```

Test *Rainbow* oraz test *Harvey'a-Colliera* na liniową zależność zmiennych nie dały podstaw do odrzucenia hipotezy zerowej $H_0$ o występowaniu liniowości, jednak test *Reset* ją odrzucił. Być może dlatego, że testuje on rozszerzanie modelu o potęgi zmiennych objaśniających, czyli model rozbudowany mógłby być o wiele lepszy niż liniowy i oryginalny model jest niewystarczający.

# Budowa modelu

## Model 1 - Regresja logistyczna

Podział danych na część treningową oraz testową. Część treningowa to będzie 85% danych, reszta to część testowa.

```{r}
dane$Outcome <- as.factor(dane$Outcome)
split <- initial_split(dane,0.85) #funkcja tworząca podział
test <- testing(split) #tworznie zbioru testowego
trening <- training(split) #tworzenie zbioru treningowego
xd <- nrow(test)
xdd <- nrow(trening)
xddd <- matrix(c(xd, xdd), ncol = 2, byrow = T)
xddd <- data.frame(xddd)
colnames(xddd) <- c("Część testowa", "Część treningowa") 
kable(xddd) 
```

Zdefiniowanie przepisu podając formułę i zbiór danych, następnie dodanie kroku normalizującego dane.

```{r}
model <- logistic_reg(mode = "classification",
                      engine = "glm")

reg_rec <- recipe(Outcome~., data = trening) %>% 
step_normalize()
```

Tworzenie przepływu danych, gdzie będzie dodany model oraz wcześniej przygotowany przepis.

```{r}
reg_wf <- workflow() %>%
  add_model(model) %>% #dodajemy model
  add_recipe(reg_rec) #dodajemy wcześniej przygotowany przepis
```

Uczenie modelu.

```{r}
reg_wf_fit <- reg_wf %>%
  fit(data = trening)
reg_wf_fit
```

Tworzenie ramki danych z predykcją, oraz zestawienie obok wartosci rzeczywistych.

```{r}
reg_pred <- predict(reg_wf_fit,test) #ramka danych z predykcją 
reg_df1 <- bind_cols(reg_pred, 'target' = test$Outcome)
colnames(reg_df1) <- c("Predykcja modelu", "Wartość rzeczywista")
kable(head(reg_df1))
```

Wizualzacja

```{r}
#| fig-cap: "Wizualizacją skuteczności przeprowadzonej klasyfikacji"
#| label: logreg


conf_reg <- conf_mat(reg_df1, truth = "Wartość rzeczywista", estimate = "Predykcja modelu")
autoplot(conf_reg, type = "heatmap")
```

LDA

```{r}
lda <- lda(Outcome~., data = dane)
lda
```

```{r}
pred_tr <- predict(lda, newdata = dane)
plot(lda, color = "skyblue")
```

```{r}
tab_tr <- bind_cols(obs_class = dane$Outcome, pred_class = pred_tr$class) |>     table()
acc_tr <- sum(diag(tab_tr))/sum(tab_tr)
acc_tr
```

```{r}
pred <- predict(lda, dane)
tab <- table(obs = dane$Outcome, pred = pred$class)
conf<- conf_mat(tab, truth = "target", estimate = ".pred_class")
autoplot(conf, type = "heatmap")
```

daleczego lda nie pca : **PCA aims to find**

**the directions of maximum variance in the data, while LDA aims to find the projection that best separates the classes in the data xdddd**

Why use LDA instead of PCA?

LDA is more effective than PCA for classification datasets because **LDA reduces the dimensionality of the data by maximizing class separability**. It is easier to draw decision boundaries for data with maximum class separability

## drzewo

```{r}
tree <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_fit <- tree %>%
  fit(Outcome ~ . , data = trening)
```

```{r}
#| fig-cap: "Ważność zmiennych"
#| label: fig-importance


var_importance <- vip(tree_fit, num_features = 8)
print(var_importance)
```

Zmienna 'Glucose' ma największy wpływ na model, na drugim i trzecim miejscu mamy 'Age' oraz 'Insulin'.

```{r}
rpart.plot(tree_fit$fit, type = 5, extra = 101, under = T , cex = 0.45, box.palette = "skyblue")
```

Na podstawie wykresu @fig-importance zmienna `Glucose` była najważniejsza dla zbioru dlatego jest głownym wierzchołkiem.

```{r}
tree_pred <- predict(tree_fit, test, type = "class")
tab_tree <- table(obs = test$Outcome, pred = t(tree_pred))
conf<- conf_mat(tab, truth = "target", estimate = ".pred_class")
autoplot(conf, type = "heatmap")
```
